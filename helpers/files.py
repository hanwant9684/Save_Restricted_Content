# Copyright (C) @Wolfy004

import os
import gc
import glob
import time
from typing import Optional

from logger import LOGGER

SIZE_UNITS = ["B", "KB", "MB", "GB", "TB", "PB"]

def get_download_path(folder_id: int, filename: str, root_dir: str = "downloads") -> str:
    folder = os.path.join(root_dir, str(folder_id))
    os.makedirs(folder, exist_ok=True)
    return os.path.join(folder, filename)


def cleanup_download(path: str) -> None:
    """
    Immediate cleanup of downloaded files (legacy function).
    Use cleanup_download_delayed() for user-tier-based cleanup with proper wait times.
    """
    try:
        if not path or path is None:
            LOGGER(__name__).debug("Cleanup skipped: path is None or empty")
            return
        
        LOGGER(__name__).info(f"Cleaning Download: {path}")
        
        if os.path.exists(path):
            os.remove(path)
        if os.path.exists(path + ".temp"):
            os.remove(path + ".temp")

        folder = os.path.dirname(path)
        if os.path.isdir(folder) and not os.listdir(folder):
            os.rmdir(folder)

    except Exception as e:
        LOGGER(__name__).error(f"Cleanup failed for {path}: {e}")

async def cleanup_download_delayed(path: str, user_id: Optional[int], db) -> None:
    """
    Cleanup downloaded files immediately after upload completes.
    Includes garbage collection to force RAM release for 512MB environments.
    The delay between downloads is now handled in the queue manager,
    not during cleanup.
    
    Args:
        path: File path to cleanup
        user_id: User ID (kept for compatibility)
        db: Database instance (kept for compatibility)
    """
    import asyncio
    
    try:
        if not path or path is None:
            LOGGER(__name__).debug("Cleanup skipped: path is None or empty")
            return
        
        LOGGER(__name__).info(f"Cleaning Download: {os.path.basename(path)}")
        
        # Immediate cleanup
        if os.path.exists(path):
            os.remove(path)
        if os.path.exists(path + ".temp"):
            os.remove(path + ".temp")
        if os.path.exists(path + ".tmp"):
            os.remove(path + ".tmp")

        folder = os.path.dirname(path)
        if os.path.isdir(folder) and not os.listdir(folder):
            os.rmdir(folder)
        
        # Force garbage collection to release RAM (critical for 512MB limit)
        gc.collect()
        
        # Yield to event loop to allow memory to be reclaimed
        await asyncio.sleep(0.1)
        
        LOGGER(__name__).info(f"âœ… Cleanup complete for {os.path.basename(path)} (RAM released)")

    except Exception as e:
        LOGGER(__name__).error(f"Cleanup failed for {path}: {e}")


def get_readable_file_size(size_in_bytes: Optional[float]) -> str:
    if size_in_bytes is None or size_in_bytes < 0:
        return "0B"

    for unit in SIZE_UNITS:
        if size_in_bytes < 1024:
            return f"{size_in_bytes:.2f} {unit}"
        size_in_bytes /= 1024

    return "File too large"


def get_readable_time(seconds: int) -> str:
    result = ""
    (days, remainder) = divmod(seconds, 86400)
    days = int(days)
    if days:
        result += f"{days}d"
    (hours, remainder) = divmod(remainder, 3600)
    hours = int(hours)
    if hours:
        result += f"{hours}h"
    (minutes, seconds) = divmod(remainder, 60)
    minutes = int(minutes)
    if minutes:
        result += f"{minutes}m"
    seconds = int(seconds)
    result += f"{seconds}s"
    return result


async def fileSizeLimit(file_size, message, action_type="download", is_premium=False):
    MAX_FILE_SIZE = 2 * 2097152000 if is_premium else 2097152000
    if file_size > MAX_FILE_SIZE:
        await message.reply(
            f"The file size exceeds the {get_readable_file_size(MAX_FILE_SIZE)} limit and cannot be {action_type}ed."
        )
        return False
    return True


def cleanup_orphaned_files() -> tuple[int, int]:
    """
    Smart cleanup of orphaned files from crashes or stuck downloads.
    
    Strategy:
    - Files in folders of users with ACTIVE downloads: NEVER touch (protects uploads too)
    - Files in folders of users with NO active download: Delete if older than 45 min
    - Always clean media files in root directory
    
    Why folder-based approach?
    - During download: file is being written (mod time updates)
    - During upload: file is only READ (mod time stays same!)
    - Using 45-min threshold alone could delete files during long uploads
    - By checking user folders, we protect entire download+upload cycle
    
    Returns: (files_removed, bytes_freed)
    """
    import time
    
    STALE_THRESHOLD = 45 * 60  # 45 minutes - matches per-file timeout
    
    try:
        files_removed = 0
        bytes_freed = 0
        now = time.time()
        
        # Get set of user IDs with active downloads
        active_user_ids = set()
        try:
            from queue_manager import download_manager
            active_user_ids = set(download_manager.active_downloads)
            if active_user_ids:
                LOGGER(__name__).debug(f"Active download users: {active_user_ids}")
        except ImportError:
            LOGGER(__name__).warning("Could not import queue_manager")
        except Exception as e:
            LOGGER(__name__).warning(f"Could not check active downloads: {e}")
        
        # Clean downloads folder - process each user folder separately
        if os.path.exists("downloads"):
            # First, get list of user folders
            try:
                user_folders = [d for d in os.listdir("downloads") 
                               if os.path.isdir(os.path.join("downloads", d))]
            except Exception as e:
                LOGGER(__name__).warning(f"Failed to list downloads folder: {e}")
                user_folders = []
            
            for user_folder in user_folders:
                user_path = os.path.join("downloads", user_folder)
                
                # Check if this user has an active download
                try:
                    user_id = int(user_folder)
                    if user_id in active_user_ids:
                        LOGGER(__name__).debug(
                            f"â­ï¸ Skipping folder for active user {user_id}"
                        )
                        continue  # Skip entire folder - user is downloading or uploading
                except ValueError:
                    pass  # Not a user ID folder, process normally
                
                # Process files in this folder (user has no active download)
                for root, dirs, files in os.walk(user_path, topdown=False):
                    for file in files:
                        filepath = os.path.join(root, file)
                        try:
                            stat = os.stat(filepath)
                            file_age = now - stat.st_mtime
                            size = stat.st_size
                            
                            # Delete if file is stale (45+ minutes old)
                            if file_age > STALE_THRESHOLD:
                                os.remove(filepath)
                                files_removed += 1
                                bytes_freed += size
                                LOGGER(__name__).info(
                                    f"Removed stale file ({file_age/60:.1f}min old): {filepath}"
                                )
                            else:
                                LOGGER(__name__).debug(
                                    f"Keeping recent file: {filepath} (age: {file_age/60:.1f}min)"
                                )
                        except Exception as e:
                            LOGGER(__name__).warning(f"Failed to check/remove {filepath}: {e}")
                    
                    # Remove empty subfolders
                    for dir in dirs:
                        dirpath = os.path.join(root, dir)
                        try:
                            if not os.listdir(dirpath):
                                os.rmdir(dirpath)
                        except:
                            pass
                
                # Remove user folder if empty
                try:
                    if not os.listdir(user_path):
                        os.rmdir(user_path)
                except:
                    pass
        
        # Cleanup media files in root directory (from crashes)
        media_extensions = ['*.MOV', '*.mov', '*.MP4', '*.mp4', '*.MKV', '*.mkv', 
                          '*.AVI', '*.avi', '*.JPG', '*.jpg', '*.JPEG', '*.jpeg',
                          '*.PNG', '*.png', '*.temp', '*.tmp']
        
        for pattern in media_extensions:
            for filepath in glob.glob(pattern):
                # Don't delete important files
                if any(x in filepath.lower() for x in ['config', 'database', 'log', 'backup', 'main', 'server']):
                    continue
                
                try:
                    size = os.path.getsize(filepath)
                    os.remove(filepath)
                    files_removed += 1
                    bytes_freed += size
                    LOGGER(__name__).info(f"Removed orphaned media file from root: {filepath}")
                except Exception as e:
                    LOGGER(__name__).warning(f"Failed to remove {filepath}: {e}")
        
        if files_removed > 0:
            LOGGER(__name__).warning(
                f"ðŸ§¹ Emergency cleanup: Removed {files_removed} orphaned files, "
                f"freed {get_readable_file_size(bytes_freed)}"
            )
        
        return files_removed, bytes_freed
        
    except Exception as e:
        LOGGER(__name__).error(f"Error during orphaned files cleanup: {e}")
        return 0, 0
